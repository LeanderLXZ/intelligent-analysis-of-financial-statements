{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3710jvsc74a57bd0274ae7b1ae6bdb01b6edc022f96e68befb0463a18757fe08f3108c854cb76e94",
   "display_name": "Python 3.7.10 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os.path import join\n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "source": [
    "# List date"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read halt information CSV\n",
    "date = '20210417'\n",
    "df_info = pd.read_csv('../../data/stocks_information/stocks_info_{}.csv'.format(date), dtype={'ticker': str})\n",
    "\n",
    "list_date_dict = {}\n",
    "for _, row in df_info.iterrows():\n",
    "    list_date_dict[row['ticker']] = row['listDate']"
   ]
  },
  {
   "source": [
    "# Delist companies list\n",
    "***"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "    TICKER STOCK_NAME DELIST_DATE\n",
       "2   000003      PT金田A  2002-06-14\n",
       "12  000013     *ST石化A  2004-09-20\n",
       "14  000015      PT中浩A  2001-10-22\n",
       "17  000018       神城A退  2020-01-07\n",
       "21  000022       深赤湾A  2018-12-26"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>TICKER</th>\n      <th>STOCK_NAME</th>\n      <th>DELIST_DATE</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2</th>\n      <td>000003</td>\n      <td>PT金田A</td>\n      <td>2002-06-14</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>000013</td>\n      <td>*ST石化A</td>\n      <td>2004-09-20</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>000015</td>\n      <td>PT中浩A</td>\n      <td>2001-10-22</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>000018</td>\n      <td>神城A退</td>\n      <td>2020-01-07</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>000022</td>\n      <td>深赤湾A</td>\n      <td>2018-12-26</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "# Read halt information CSV\n",
    "date = '20210417'\n",
    "df_info = pd.read_csv('../../data/stocks_information/stocks_info_{}.csv'.format(date))\n",
    "\n",
    "# Copy halt dataframe\n",
    "df_delist = df_info.copy()\n",
    "\n",
    "# Get delist stocks\n",
    "# df_delist = df_delist[df_delist['listStatusCD'] == 'DE']\n",
    "df_delist = df_delist[(df_delist['listStatusCD'] == 'DE') & (df_delist['ticker'].apply(lambda x: len(x) == 6))]\n",
    "\n",
    "# Drop columns\n",
    "df_delist = df_delist[['ticker', 'secShortName', 'delistDate']]\n",
    "\n",
    "# Normalize ticker\n",
    "df_delist['ticker'] = df_delist['ticker'].apply(lambda x: str(x).zfill(6))\n",
    "\n",
    "# Delete duplicates (with different halt dates)\n",
    "df_delist.drop_duplicates(inplace=True)\n",
    "\n",
    "# Normalize ticker\n",
    "df_delist = df_delist.rename(columns={\n",
    "    'ticker': 'TICKER',\n",
    "    'secShortName': 'STOCK_NAME',\n",
    "    'delistDate': 'DELIST_DATE'\n",
    "})\n",
    "df_delist = df_delist.sort_values(by='TICKER')\n",
    "\n",
    "# Save dataframe\n",
    "df_delist.to_csv('../../data/stocks_information/delist_{}.csv'.format(date), index=False)\n",
    "df_delist.to_excel('../../data/stocks_information/delist_{}.xlsx'.format(date), index=False)\n",
    "\n",
    "df_delist.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(136, 3)"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "df_delist.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/leander/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3166: DtypeWarning: Columns (3) have mixed types.Specify dtype option on import or set low_memory=False.\n  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "      TICKER STOCK_NAME DELIST_DATE\n",
       "819   000018       神城A退  2020-01-07\n",
       "1100  000024       招商地产  2015-12-30\n",
       "1581  000033        新都退  2017-07-07\n",
       "4094  000406       石油大明  2006-04-21\n",
       "4631  000418       小天鹅A  2019-06-21"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>TICKER</th>\n      <th>STOCK_NAME</th>\n      <th>DELIST_DATE</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>819</th>\n      <td>000018</td>\n      <td>神城A退</td>\n      <td>2020-01-07</td>\n    </tr>\n    <tr>\n      <th>1100</th>\n      <td>000024</td>\n      <td>招商地产</td>\n      <td>2015-12-30</td>\n    </tr>\n    <tr>\n      <th>1581</th>\n      <td>000033</td>\n      <td>新都退</td>\n      <td>2017-07-07</td>\n    </tr>\n    <tr>\n      <th>4094</th>\n      <td>000406</td>\n      <td>石油大明</td>\n      <td>2006-04-21</td>\n    </tr>\n    <tr>\n      <th>4631</th>\n      <td>000418</td>\n      <td>小天鹅A</td>\n      <td>2019-06-21</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "# # Read halt information CSV\n",
    "# date = '20210417'\n",
    "# df_halt = pd.read_csv('../../data/stocks_information/halt_{}.csv'.format(date))\n",
    "\n",
    "# # Copy halt dataframe\n",
    "# df_delist = df_halt.copy()\n",
    "\n",
    "# # Get delist stocks\n",
    "# df_delist = df_delist[df_delist['listStatusCD'] == 'DE']\n",
    "\n",
    "# # Drop columns\n",
    "# df_delist.drop(columns=['secID', 'haltBeginTime', 'haltEndTime', 'exchangeCD', 'listStatusCD', 'assetClass'], inplace=True)\n",
    "\n",
    "# # Normalize ticker\n",
    "# df_delist['ticker'] = df_delist['ticker'].apply(lambda x: str(x).zfill(6))\n",
    "\n",
    "# # Delete duplicates (with different halt dates)\n",
    "# df_delist.drop_duplicates(inplace=True)\n",
    "\n",
    "# # Normalize ticker\n",
    "# df_delist = df_delist.rename(columns={\n",
    "#     'ticker': 'TICKER',\n",
    "#     'secShortName': 'STOCK_NAME',\n",
    "#     'delistDate': 'DELIST_DATE'\n",
    "# })\n",
    "# df_delist = df_delist.sort_values(by='TICKER')\n",
    "\n",
    "# # Save dataframe\n",
    "# df_delist.to_csv('../../data/stocks_information/delist_{}.csv'.format(date), index=False)\n",
    "# df_delist.to_excel('../../data/stocks_information/delist_{}.xlsx'.format(date), index=False)\n",
    "\n",
    "# df_delist.head()"
   ]
  },
  {
   "source": [
    "# ST companies list\n",
    "***"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read ST information CSV\n",
    "date = '20210317'\n",
    "df_st = pd.read_csv('../../data/stocks_information/st_{}.csv'.format(date), index_col=0)\n",
    "\n",
    "# Normalize ticker\n",
    "df_st['ticker'] = df_st['ticker'].apply(lambda x: str(x).zfill(6))\n",
    "\n",
    "# Get the set of stocks\n",
    "stocks = set(i for i in zip(df_st['ticker'], df_st['STflg'], df_st['tradeAbbrName']))\n",
    "\n",
    "# Extract start date and end date of the ST duration\n",
    "df_st_duration = pd.DataFrame(columns=('TICKER', 'STOCK_NAME', 'START_DATE', 'END_DATE', 'ST_FLAG'))\n",
    "for t, s, n in stocks:\n",
    "    df_i = df_st[(df_st['ticker'] == t) & (df_st['STflg'] == s)]\n",
    "    dates_i = df_i['tradeDate'].sort_values().values\n",
    "    start_date, end_date = dates_i[0], dates_i[-1]\n",
    "    df_st_duration = df_st_duration.append(pd.DataFrame({\n",
    "        'TICKER': [t], \n",
    "        'STOCK_NAME': [n], \n",
    "        'START_DATE': [start_date], \n",
    "        'END_DATE': [end_date], \n",
    "        'ST_FLAG': [s]\n",
    "    }), ignore_index=True)\n",
    "df_st_duration = df_st_duration.sort_values(by='TICKER')\n",
    "\n",
    "# Save dataframe\n",
    "df_st_duration.to_csv('../../data/stocks_information/st_duration_{}.csv'.format(date), index=False)\n",
    "df_st_duration.to_excel('../../data/stocks_information/st_duration_{}.xlsx'.format(date), index=False)\n",
    "\n",
    "df_st_duration.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(df_st['STflg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('S: ', len(set(df_st[df_st['STflg'] == 'S']['secID'])))\n",
    "print('*: ', len(set(df_st[df_st['STflg'] == '*']['secID'])))\n",
    "print('ST: ', len(set(df_st[df_st['STflg'] == 'ST']['secID'])))\n",
    "print('*ST: ', len(set(df_st[df_st['STflg'] == '*ST']['secID'])))\n",
    "print('SST: ', len(set(df_st[df_st['STflg'] == 'SST']['secID'])))\n",
    "print('S*ST: ', len(set(df_st[df_st['STflg'] == 'S*ST']['secID'])))\n",
    "print('Total: ', len(set(df_st['secID'])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_path = 'data/financial_statements_pit/original/csv'\n",
    "\n",
    "for cn_name, en_name in self.cn_to_en.items():\n",
    "    print(cn_name, en_name)\n",
    "    df = pd.read_csv(join(orig_path, cn_name) + '.csv')\n",
    "\n",
    "    # Nomalize ticker\n",
    "    df['ticker'] = df['ticker'].apply(lambda x: str(x).zfill(6))\n",
    "\n",
    "    # Delete Companies\n",
    "    df.drop(df[df['ticker'].apply(\n",
    "        lambda x: (x[0] in ['9', 'A', '2']) | (x in self.drop_list)\n",
    "    )].index, inplace=True\n",
    "\n",
    "    df = df.sort_values(\n",
    "        by=['ticker', 'endDate', 'endDateRep', 'actPubtime', 'fiscalPeriod'], \n",
    "        ascending=[True, True, True, True, True]\n",
    "    )"
   ]
  },
  {
   "source": [
    "# Preprocessing\n",
    "***"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess import DataPreprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "income_bank\n",
      "income_insurance\n",
      "income_security\n",
      "income_industry\n",
      "balance_bank\n",
      "balance_insurance\n",
      "balance_security\n",
      "balance_industry\n",
      "cash_flow_bank\n",
      "cash_flow_insurance\n",
      "cash_flow_security\n",
      "cash_flow_industry\n",
      "----------------------------------------------------------------------\n",
      "income_bank\n",
      "----------------------------------------------------------------------\n",
      "income_insurance\n",
      "----------------------------------------------------------------------\n",
      "income_security\n",
      "----------------------------------------------------------------------\n",
      "income_industry\n",
      "----------------------------------------------------------------------\n",
      "balance_bank\n",
      "----------------------------------------------------------------------\n",
      "balance_insurance\n",
      "----------------------------------------------------------------------\n",
      "balance_security\n",
      "----------------------------------------------------------------------\n",
      "balance_industry\n",
      "----------------------------------------------------------------------\n",
      "cash_flow_bank\n",
      "----------------------------------------------------------------------\n",
      "cash_flow_insurance\n",
      "----------------------------------------------------------------------\n",
      "cash_flow_security\n",
      "----------------------------------------------------------------------\n",
      "cash_flow_industry\n"
     ]
    }
   ],
   "source": [
    "delete_list = set(df_delist['TICKER'])\n",
    "\n",
    "DataPreprocess(\n",
    "    '../../data/financial_statements/',\n",
    "    '../../data/financial_statements_202009/',\n",
    "    delete_list\n",
    ").process_l1('2000-01-01', '2020-10-01')\n",
    "\n",
    "DataPreprocess(\n",
    "    '../../data/financial_statements_202009/',\n",
    "    '../../data/financial_statements_202009/',\n",
    "    delete_list\n",
    ").process_l2()"
   ]
  },
  {
   "source": [
    "# Compare PIT"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "File: income_bank\n",
      "----------\n",
      "Non-PIT shape: (1120, 2)\n",
      "PIT shape: (1114, 2)\n",
      "----------\n",
      "In Non-PIT but not in PIT:\n",
      "======================================================================\n",
      "File: income_insurance\n",
      "----------\n",
      "Non-PIT shape: (216, 2)\n",
      "PIT shape: (216, 2)\n",
      "======================================================================\n",
      "File: income_security\n",
      "----------\n",
      "Non-PIT shape: (1192, 2)\n",
      "PIT shape: (1191, 2)\n",
      "----------\n",
      "In Non-PIT but not in PIT:\n",
      "======================================================================\n",
      "File: income_industry\n",
      "----------\n",
      "Non-PIT shape: (125766, 2)\n",
      "PIT shape: (124398, 2)\n",
      "----------\n",
      "In Non-PIT but not in PIT:\n",
      "----------\n",
      "In PIT but not in Non-PIT:\n",
      "======================================================================\n",
      "File: balance_bank\n",
      "----------\n",
      "Non-PIT shape: (1033, 2)\n",
      "PIT shape: (1033, 2)\n",
      "======================================================================\n",
      "File: balance_insurance\n",
      "----------\n",
      "Non-PIT shape: (188, 2)\n",
      "PIT shape: (188, 2)\n",
      "======================================================================\n",
      "File: balance_security\n",
      "----------\n",
      "Non-PIT shape: (1121, 2)\n",
      "PIT shape: (1122, 2)\n",
      "----------\n",
      "In PIT but not in Non-PIT:\n",
      "======================================================================\n",
      "File: balance_industry\n",
      "----------\n",
      "Non-PIT shape: (120154, 2)\n",
      "PIT shape: (121376, 2)\n",
      "----------\n",
      "In PIT but not in Non-PIT:\n",
      "======================================================================\n",
      "File: cash_flow_bank\n",
      "----------\n",
      "Non-PIT shape: (1091, 2)\n",
      "PIT shape: (1085, 2)\n",
      "----------\n",
      "In Non-PIT but not in PIT:\n",
      "======================================================================\n",
      "File: cash_flow_insurance\n",
      "----------\n",
      "Non-PIT shape: (214, 2)\n",
      "PIT shape: (27908, 2)\n",
      "----------\n",
      "In Non-PIT but not in PIT:\n",
      "----------\n",
      "In PIT but not in Non-PIT:\n",
      "======================================================================\n",
      "File: cash_flow_security\n",
      "----------\n",
      "Non-PIT shape: (1230, 2)\n",
      "PIT shape: (1229, 2)\n",
      "----------\n",
      "In Non-PIT but not in PIT:\n",
      "======================================================================\n",
      "File: cash_flow_industry\n",
      "----------\n",
      "Non-PIT shape: (108919, 2)\n",
      "PIT shape: (124343, 2)\n",
      "----------\n",
      "In Non-PIT but not in PIT:\n",
      "----------\n",
      "In PIT but not in Non-PIT:\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# from os.path import join\n",
    "\n",
    "# # Check if directories exit or not\n",
    "# def check_dirs(path_list):\n",
    "#     for dir_path in path_list:\n",
    "#         if not os.path.isdir(dir_path):\n",
    "#             os.makedirs(dir_path)\n",
    "\n",
    "# df_cn_en = pd.read_csv('../../data/financial_statements/cn_to_en.csv')\n",
    "# cn_to_en = {c: e for c, e in zip(df_cn_en['cn_name'], df_cn_en['en_name'])}\n",
    "\n",
    "# save_path = '../../data/comparison'\n",
    "# check_dirs([save_path])\n",
    "# check_dirs([join(save_path, 'in-PIT-not-in-Non-PIT')])\n",
    "# check_dirs([join(save_path, 'in-Non-PIT-not-in-PIT')])\n",
    "\n",
    "# for cn_name, en_name in cn_to_en.items():\n",
    "#     print('File:', en_name)\n",
    "\n",
    "#     df_non_pit_orig = pd.read_csv('../../data/financial_statements/normalized_l2/csv/' + en_name + '.csv')\n",
    "#     df_pit_orig = pd.read_csv('../../data/financial_statements_pit/normalized_l2/csv/' + en_name + '_pit.csv')\n",
    "\n",
    "#     # Nomalize ticker\n",
    "#     df_non_pit_orig['ticker'] = df_non_pit_orig['ticker'].apply(lambda x: str(x).zfill(6))\n",
    "#     df_pit_orig['ticker'] = df_pit_orig['ticker'].apply(lambda x: str(x).zfill(6))\n",
    "    \n",
    "#     df_non_pit = df_non_pit_orig[['ticker', 'endDate']]\n",
    "#     df_pit = df_pit_orig[['ticker', 'endDate']]\n",
    "\n",
    "#     set_non_pit = set()\n",
    "#     for _, row in df_non_pit.iterrows():\n",
    "#         set_non_pit.add((\n",
    "#             row['ticker'], \n",
    "#             row['endDate']\n",
    "#         ))\n",
    "#     set_pit = set()\n",
    "#     for _, row in df_pit.iterrows():\n",
    "#         set_pit.add((\n",
    "#             row['ticker'], \n",
    "#             row['endDate']\n",
    "#         ))\n",
    "\n",
    "#     print('-' * 10)\n",
    "#     print('Non-PIT shape:', df_non_pit.shape)\n",
    "#     print('PIT shape:', df_pit.shape)\n",
    "#     if set_non_pit - set_pit:\n",
    "#         print('-' * 10)\n",
    "#         print('In Non-PIT but not in PIT:')\n",
    "#         df_in_non_pit = []\n",
    "#         for i in set_non_pit - set_pit:\n",
    "#             # print(i)\n",
    "#             df_in_non_pit.append(\n",
    "#                 df_non_pit_orig[(df_non_pit_orig['ticker'] == i[0]) & (df_non_pit_orig['endDate'] == i[1])])\n",
    "#         df_in_non_pit = pd.concat(df_in_non_pit)\n",
    "#         df_in_non_pit = df_in_non_pit.sort_values(by='ticker')\n",
    "#         df_in_non_pit.to_excel(join(join(save_path, 'in-Non-PIT-not-in-PIT'), en_name + '.xlsx'), index=False)\n",
    "#     if set_pit - set_non_pit:\n",
    "#         print('-' * 10)\n",
    "#         print('In PIT but not in Non-PIT:')\n",
    "#         df_in_pit = []\n",
    "#         for i in set_pit - set_non_pit:\n",
    "#             # print(i)\n",
    "#             df_in_pit.append(\n",
    "#                 df_pit_orig[(df_pit_orig['ticker'] == i[0]) & (df_pit_orig['endDate'] == i[1])])\n",
    "#         df_in_pit = pd.concat(df_in_pit)\n",
    "#         df_in_pit = df_in_pit.sort_values(by='ticker')\n",
    "#         df_in_pit.to_excel(join(join(save_path, 'in-PIT-not-in-Non-PIT'), en_name + '.xlsx'), index=False)\n",
    "#     print('=' * 70)"
   ]
  },
  {
   "source": [
    "# Find zeros\n",
    "***"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "File: income_bank\n",
      "File: income_insurance\n",
      "File: income_security\n",
      "File: income_industry\n",
      "File: balance_bank\n",
      "File: balance_insurance\n",
      "File: balance_security\n",
      "File: balance_industry\n",
      "File: cash_flow_bank\n",
      "File: cash_flow_insurance\n",
      "File: cash_flow_security\n",
      "File: cash_flow_industry\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from os.path import join\n",
    "\n",
    "# Check if directories exit or not\n",
    "def check_dirs(path_list):\n",
    "    for dir_path in path_list:\n",
    "        if not os.path.isdir(dir_path):\n",
    "            os.makedirs(dir_path)\n",
    "\n",
    "data_path = '../../data/financial_statements_202009/'\n",
    "\n",
    "df_cn_en = pd.read_csv(join(data_path, 'cn_to_en.csv'))\n",
    "cn_to_en = {c: e for c, e in zip(df_cn_en['cn_name'], df_cn_en['en_name'])}\n",
    "\n",
    "save_path = join(data_path, 'normalized_l2/zeros')\n",
    "check_dirs([save_path])\n",
    "\n",
    "for en_name in cn_to_en.values():\n",
    "    print('File:', en_name)\n",
    "    df = pd.read_csv(join(join(data_path, 'normalized_l2/csv'), en_name) + '.csv')\n",
    "    df_zero = []\n",
    "    for _, row in df.iterrows():\n",
    "        for k, v in row.items():\n",
    "            if v == 0:\n",
    "                df_zero.append(row)\n",
    "    if df_zero:\n",
    "        df_zero = pd.concat(df_zero, axis=1).T\n",
    "        df_zero.to_excel(join(save_path, en_name) + '.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from os.path import join\n",
    "\n",
    "# # Check if directories exit or not\n",
    "# def check_dirs(path_list):\n",
    "#     for dir_path in path_list:\n",
    "#         if not os.path.isdir(dir_path):\n",
    "#             os.makedirs(dir_path)\n",
    "\n",
    "# data_path = '../../data/financial_statements_pit/'\n",
    "\n",
    "# df_cn_en = pd.read_csv(join(data_path, 'cn_to_en.csv'))\n",
    "# cn_to_en = {c: e for c, e in zip(df_cn_en['cn_name'], df_cn_en['en_name'])}\n",
    "\n",
    "# save_path = join(data_path, 'normalized_l2/zeros')\n",
    "# check_dirs([save_path])\n",
    "\n",
    "# for en_name in cn_to_en.values():\n",
    "#     print('File:', en_name)\n",
    "#     df = pd.read_csv(join(join(data_path, 'normalized_l2/csv'), en_name) + '.csv')\n",
    "#     df_zero = []\n",
    "#     for _, row in df.iterrows():\n",
    "#         for k, v in row.items():\n",
    "#             if v == 0:\n",
    "#                 df_zero.append(row)\n",
    "#     if df_zero:\n",
    "#         df_zero = pd.concat(df_zero, axis=1).T\n",
    "#         df_zero.to_excel(join(save_path, en_name) + '.xlsx', index=False)"
   ]
  },
  {
   "source": [
    "# Check Q3\n",
    "***"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from os.path import join\n",
    "\n",
    "# Check if directories exit or not\n",
    "def check_dirs(path_list):\n",
    "    for dir_path in path_list:\n",
    "        if not os.path.isdir(dir_path):\n",
    "            os.makedirs(dir_path)\n",
    "\n",
    "def check_Q3_fiscal_period(data_path):\n",
    "    df_cn_en = pd.read_csv(join(data_path, 'cn_to_en.csv'))\n",
    "    cn_to_en = {c: e for c, e in zip(df_cn_en['cn_name'], df_cn_en['en_name'])}\n",
    "\n",
    "    save_path = join(data_path, 'Q3-3-not-9_endDate_endDateRep')\n",
    "    check_dirs([save_path])\n",
    "\n",
    "    for en_name in cn_to_en.values():\n",
    "        print('File:', en_name)\n",
    "        df = pd.read_csv(join(join('../../data/financial_statements/', 'original/csv'), en_name) + '.csv')\n",
    "        df_Q3_3 = df[(df['reportType']=='Q3') | (df['reportType']=='CQ3')].groupby(\n",
    "            ['ticker', 'endDate', 'endDateRep']).apply(\n",
    "                lambda x: 9 not in set(x['fiscalPeriod']))\n",
    "        df_Q3_3_not_9 = []\n",
    "        for t, ed, edr in df_Q3_3[df_Q3_3.values].index.to_list():\n",
    "            df_Q3_3_not_9.append(\n",
    "                df[(df['ticker']==t) \\\n",
    "                & (df['endDate']==ed) \\\n",
    "                & (df['endDateRep']==edr) \\\n",
    "                & (df['mergedFlag']==1)])\n",
    "        if df_Q3_3_not_9:\n",
    "            df_Q3_3_not_9 = pd.concat(df_Q3_3_not_9)\n",
    "            df_Q3_3_not_9.to_excel(join(save_path, en_name) + '.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "File: income_bank\n",
      "File: income_insurance\n",
      "File: income_security\n",
      "File: income_industry\n",
      "File: balance_bank\n",
      "File: balance_insurance\n",
      "File: balance_security\n",
      "File: balance_industry\n",
      "File: cash_flow_bank\n",
      "File: cash_flow_insurance\n",
      "File: cash_flow_security\n",
      "File: cash_flow_industry\n"
     ]
    }
   ],
   "source": [
    "check_Q3_fiscal_period('../../data/financial_statements_202009/')\n",
    "# check_Q3_fiscal_period('../../data/financial_statements_pit/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from os.path import join\n",
    "\n",
    "# Check if directories exit or not\n",
    "def check_dirs(path_list):\n",
    "    for dir_path in path_list:\n",
    "        if not os.path.isdir(dir_path):\n",
    "            os.makedirs(dir_path)\n",
    "\n",
    "def check_Q3_fiscal_period(data_path):\n",
    "    df_cn_en = pd.read_csv(join(data_path, 'cn_to_en.csv'))\n",
    "    cn_to_en = {c: e for c, e in zip(df_cn_en['cn_name'], df_cn_en['en_name'])}\n",
    "\n",
    "    save_path = join(data_path, 'Q3-3-not-9_endDate')\n",
    "    check_dirs([save_path])\n",
    "\n",
    "    for en_name in cn_to_en.values():\n",
    "        print('File:', en_name)\n",
    "        df = pd.read_csv(join(join('../../data/financial_statements/', 'original/csv'), en_name) + '.csv')\n",
    "        df_Q3_3 = df[(df['reportType']=='Q3') | (df['reportType']=='CQ3')].groupby(\n",
    "            ['ticker', 'endDate']).apply(\n",
    "                lambda x: 9 not in set(x['fiscalPeriod']))\n",
    "        df_Q3_3_not_9 = []\n",
    "        for t, ed in df_Q3_3[df_Q3_3.values].index.to_list():\n",
    "            df_Q3_3_not_9.append(\n",
    "                df[(df['ticker']==t) \\\n",
    "                & (df['endDate']==ed) \\\n",
    "                & (df['mergedFlag']==1)])\n",
    "        if df_Q3_3_not_9:\n",
    "            df_Q3_3_not_9 = pd.concat(df_Q3_3_not_9)\n",
    "            df_Q3_3_not_9.to_excel(join(save_path, en_name) + '.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "File: income_bank\n",
      "File: income_insurance\n",
      "File: income_security\n",
      "File: income_industry\n",
      "File: balance_bank\n",
      "File: balance_insurance\n",
      "File: balance_security\n",
      "File: balance_industry\n",
      "File: cash_flow_bank\n",
      "File: cash_flow_insurance\n",
      "File: cash_flow_security\n",
      "File: cash_flow_industry\n"
     ]
    }
   ],
   "source": [
    "check_Q3_fiscal_period('../../data/financial_statements_202009/')\n",
    "# check_Q3_fiscal_period('../../data/financial_statements_pit/')"
   ]
  },
  {
   "source": [
    "# Time series statistics\n",
    "***"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# income - year"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if directories exit or not\n",
    "def check_dirs(path_list):\n",
    "    for dir_path in path_list:\n",
    "        if not os.path.isdir(dir_path):\n",
    "            os.makedirs(dir_path)\n",
    "\n",
    "def generate_year_revenue_sheets(df_income, data_path, file_name):\n",
    "\n",
    "    df_income['ticker'] = df_income['ticker'].apply(lambda x: str(x).zfill(6))\n",
    "    df_income = df_income[df_income['endDate'].apply(lambda x: str(x)[-6:]=='-12-31')]\n",
    "    df_income.drop_duplicates(subset=['ticker', 'endDate'], keep='last', inplace=True)\n",
    "    df_income['endDate'] = df_income['endDate'].apply(lambda x: int(x[:4]))\n",
    "\n",
    "    df_income.set_index(['endDate', 'ticker'], inplace=True)\n",
    "    df_income.index.names = [None, 'TICKER']\n",
    "\n",
    "    df_income_r = df_income[['revenue']]\n",
    "    df_income_r = df_income_r.unstack(1).T\n",
    "    df_income_r.reset_index(inplace=True)\n",
    "    df_income_r = df_income_r.drop(columns=['level_0'])\n",
    "    df_income_r.set_index('TICKER', inplace=True)\n",
    "\n",
    "    df_income_r_count_list = pd.DataFrame(columns=('TICKER', 'COUNTS', 'YEARS'))\n",
    "    df_income_r_not_ct_list = pd.DataFrame(columns=('TICKER', 'YEARS'))\n",
    "    last_year = int(sorted(df_income_r.columns)[-1])\n",
    "\n",
    "    for ticker, row in df_income_r.iterrows():\n",
    "        row = row.notnull()\n",
    "        counts = sum(row)\n",
    "        years_list = []\n",
    "        for k in row.keys():\n",
    "            if row[k]:\n",
    "                years_list.append(k)\n",
    "        df_income_r_count_list = df_income_r_count_list.append(\n",
    "            pd.DataFrame({'TICKER': [ticker], 'COUNTS': [counts], 'YEARS': [years_list]}), ignore_index=True)\n",
    "\n",
    "        if not years_list:\n",
    "            print('No data:', ticker)\n",
    "            df_income_r_not_ct_list = df_income_r_not_ct_list.append(\n",
    "                pd.DataFrame({'TICKER': [ticker], 'YEARS': [[]]}), ignore_index=True)\n",
    "        # elif years_list[-1] - years_list[0] != counts - 1:\n",
    "        elif last_year - years_list[0] != counts - 1:\n",
    "            df_income_r_not_ct_list = df_income_r_not_ct_list.append(\n",
    "                pd.DataFrame({'TICKER': [ticker], 'YEARS': [years_list]}), ignore_index=True)\n",
    "\n",
    "    save_path = join(data_path, 'revenue')\n",
    "    check_dirs([save_path])\n",
    "    df_income_r.to_excel(join(save_path, file_name + '.xlsx'), index=True)\n",
    "    df_income_r_count_list.to_excel(join(save_path, file_name + '_counts.xlsx'), index=False)\n",
    "    df_income_r_not_ct_list.to_excel(join(save_path, file_name + '_discontinuous.xlsx'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "ipykernel_launcher:11: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\nipykernel_launcher:12: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "data_path = '../../data/financial_statements_202009/normalized_l2/'\n",
    "df_1 = pd.read_csv(join(join(data_path, 'csv'), 'income_bank.csv'))[['ticker', 'endDate', 'revenue']]\n",
    "df_2 = pd.read_csv(join(join(data_path, 'csv'), 'income_insurance.csv'))[['ticker', 'endDate', 'revenue']]\n",
    "df_3 = pd.read_csv(join(join(data_path, 'csv'), 'income_security.csv'))[['ticker', 'endDate', 'revenue']]\n",
    "df_4 = pd.read_csv(join(join(data_path, 'csv'), 'income_industry.csv'))[['ticker', 'endDate', 'revenue']]\n",
    "df = pd.concat([df_1, df_2, df_3, df_4])\n",
    "generate_year_revenue_sheets(df, data_path, 'revenue_year')\n",
    "\n",
    "# data_path = '../../data/financial_statements_pit/normalized_l2/'\n",
    "# df_1 = pd.read_csv(join(join(data_path, 'csv'), 'income_bank_pit.csv'))[['ticker', 'endDate', 'revenue']]\n",
    "# df_2 = pd.read_csv(join(join(data_path, 'csv'), 'income_insurance_pit.csv'))[['ticker', 'endDate', 'revenue']]\n",
    "# df_3 = pd.read_csv(join(join(data_path, 'csv'), 'income_security_pit.csv'))[['ticker', 'endDate', 'revenue']]\n",
    "# df_4 = pd.read_csv(join(join(data_path, 'csv'), 'income_industry_pit.csv'))[['ticker', 'endDate', 'revenue']]\n",
    "# df = pd.concat([df_1, df_2, df_3, df_4])\n",
    "# generate_year_revenue_sheets(df, data_path, 'revenue_year_pit')"
   ]
  },
  {
   "source": [
    "# industry t-revenue - revenue"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_income_industry = pd.read_csv('../../data/financial_statements_202009/normalized_l2/csv/income_industry.csv')\n",
    "col_name = df_income_industry.columns.tolist()\n",
    "col_name.insert(col_name.index('revenue') + 1, 'tRevenue-revenue')\n",
    "df_income_industry = df_income_industry.reindex(columns=col_name)\n",
    "df_income_industry['tRevenue-revenue'] = df_income_industry['tRevenue'] - df_income_industry['revenue']\n",
    "df_income_industry.to_excel('../../data/financial_statements_202009/normalized_l2/revenue/income_industry_t_revenue.xlsx')\n",
    "\n",
    "# df_income_industry = pd.read_csv('../../data/financial_statements_pit/normalized_l2/csv/income_industry_pit.csv')\n",
    "# col_name = df_income_industry.columns.tolist()\n",
    "# col_name.insert(col_name.index('revenue') + 1, 'tRevenue-revenue')\n",
    "# df_income_industry = df_income_industry.reindex(columns=col_name)\n",
    "# df_income_industry['tRevenue-revenue'] = df_income_industry['tRevenue'] - df_income_industry['revenue']\n",
    "# df_income_industry.to_excel('../../data/financial_statements_pit/normalized_l2/revenue/income_industry_pit_t_revenue.xlsx')"
   ]
  },
  {
   "source": [
    "# income - quarter"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if directories exit or not\n",
    "def check_dirs(path_list):\n",
    "    for dir_path in path_list:\n",
    "        if not os.path.isdir(dir_path):\n",
    "            os.makedirs(dir_path)\n",
    "\n",
    "def generate_quarter_revenue_sheets(df_income, data_path, file_name):\n",
    "\n",
    "    df_income['ticker'] = df_income['ticker'].apply(lambda x: str(x).zfill(6))\n",
    "    df_income.drop_duplicates(subset=['ticker', 'endDate'], keep='last', inplace=True)\n",
    "    df_income['endDate'] = df_income['endDate'].apply(lambda x: x[:7])\n",
    "\n",
    "    df_income.set_index(['endDate', 'ticker'], inplace=True)\n",
    "    df_income.index.names = [None, 'TICKER']\n",
    "\n",
    "    df_income_r = df_income[['revenue']]\n",
    "    df_income_r = df_income_r.unstack(1).T\n",
    "    df_income_r.reset_index(inplace=True)\n",
    "    df_income_r = df_income_r.drop(columns=['level_0'])\n",
    "    df_income_r.set_index('TICKER', inplace=True)\n",
    "\n",
    "    df_income_r_count_list = pd.DataFrame(columns=('TICKER', 'COUNTS', 'QUARTERS'))\n",
    "    df_income_r_not_ct_list = pd.DataFrame(columns=('TICKER', 'QUARTERS'))\n",
    "    quarters_all = sorted(df_income_r.columns)\n",
    "    last_quarter = quarters_all[-1]\n",
    "\n",
    "    for ticker, row in df_income_r.iterrows():\n",
    "        row = row.notnull()\n",
    "        counts = sum(row)\n",
    "        quarters_list = []\n",
    "        for k in row.keys():\n",
    "            if row[k]:\n",
    "                quarters_list.append(k)\n",
    "        df_income_r_count_list = df_income_r_count_list.append(\n",
    "            pd.DataFrame({'TICKER': [ticker], 'COUNTS': [counts], 'QUARTERS': [quarters_list]}), ignore_index=True)\n",
    "\n",
    "        if not quarters_list:\n",
    "            print('No data:', ticker)\n",
    "            df_income_r_not_ct_list = df_income_r_not_ct_list.append(\n",
    "                pd.DataFrame({'TICKER': [ticker], 'QUARTERS': [[]]}), ignore_index=True)\n",
    "        # elif quarters_list[-1] - quarters_list[0] != counts - 1:\n",
    "        elif len(quarters_all) - quarters_all.index(quarters_list[0]) != counts:\n",
    "            df_income_r_not_ct_list = df_income_r_not_ct_list.append(\n",
    "                pd.DataFrame({'TICKER': [ticker], 'QUARTERS': [quarters_list]}), ignore_index=True)\n",
    "\n",
    "    save_path = join(data_path, 'revenue')\n",
    "    check_dirs([save_path])\n",
    "    df_income_r.to_excel(join(save_path, file_name + '.xlsx'), index=True)\n",
    "    df_income_r_count_list.to_excel(join(save_path, file_name + '_counts.xlsx'), index=False)\n",
    "    df_income_r_not_ct_list.to_excel(join(save_path, file_name + '_discontinuous.xlsx'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../../data/financial_statements_202009/normalized_l2/'\n",
    "df_1 = pd.read_csv(join(join(data_path, 'csv'), 'income_bank.csv'))[['ticker', 'endDate', 'revenue']]\n",
    "df_2 = pd.read_csv(join(join(data_path, 'csv'), 'income_insurance.csv'))[['ticker', 'endDate', 'revenue']]\n",
    "df_3 = pd.read_csv(join(join(data_path, 'csv'), 'income_security.csv'))[['ticker', 'endDate', 'revenue']]\n",
    "df_4 = pd.read_csv(join(join(data_path, 'csv'), 'income_industry.csv'))[['ticker', 'endDate', 'revenue']]\n",
    "df = pd.concat([df_1, df_2, df_3, df_4])\n",
    "generate_quarter_revenue_sheets(df, data_path, 'revenue_quarter')\n",
    "\n",
    "# data_path = '../../data/financial_statements_pit/normalized_l2/'\n",
    "# df_1 = pd.read_csv(join(join(data_path, 'csv'), 'income_bank_pit.csv'))[['ticker', 'endDate', 'revenue']]\n",
    "# df_2 = pd.read_csv(join(join(data_path, 'csv'), 'income_insurance_pit.csv'))[['ticker', 'endDate', 'revenue']]\n",
    "# df_3 = pd.read_csv(join(join(data_path, 'csv'), 'income_security_pit.csv'))[['ticker', 'endDate', 'revenue']]\n",
    "# df_4 = pd.read_csv(join(join(data_path, 'csv'), 'income_industry_pit.csv'))[['ticker', 'endDate', 'revenue']]\n",
    "# df = pd.concat([df_1, df_2, df_3, df_4])\n",
    "# generate_quarter_revenue_sheets(df, data_path, 'revenue_quarter_pit')"
   ]
  },
  {
   "source": [
    "# Check data absence in three statements\n",
    "***"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from os.path import join\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "date = '20210417'\n",
    "df_info = pd.read_csv('../../data/stocks_information/stocks_info_{}.csv'.format(date), dtype={'ticker': str})\n",
    "\n",
    "new_listed = set(df_info[(df_info['listStatusCD'] == 'UN') | (df_info['listStatusCD'] == 'O')]['ticker'])\n",
    "new_listed = [i for i in new_listed if i[0] not in ['A', '9', '2']]\n",
    "\n",
    "list_date_dict = {}\n",
    "for _, row in df_info.iterrows():\n",
    "    list_date_dict[row['ticker']] = row['listDate']\n",
    "\n",
    "# Check if directories exit or not\n",
    "def check_dirs(path_list):\n",
    "    for dir_path in path_list:\n",
    "        if not os.path.isdir(dir_path):\n",
    "            os.makedirs(dir_path)\n",
    "\n",
    "def get_concat_df(data_path, name, post_fix=''):\n",
    "    df_1 = pd.read_csv(join(join(data_path, 'csv'), name + '_bank' + post_fix + '.csv'))[['ticker', 'endDate']]\n",
    "    df_2 = pd.read_csv(join(join(data_path, 'csv'), name + '_insurance' + post_fix + '.csv'))[['ticker', 'endDate']]\n",
    "    df_3 = pd.read_csv(join(join(data_path, 'csv'), name + '_security' + post_fix + '.csv'))[['ticker', 'endDate']]\n",
    "    df_4 = pd.read_csv(join(join(data_path, 'csv'), name + '_industry' + post_fix + '.csv'))[['ticker', 'endDate']]\n",
    "    df = pd.concat([df_1, df_2, df_3, df_4])\n",
    "    df['ticker'] = df['ticker'].apply(lambda x: str(x).zfill(6))\n",
    "\n",
    "    df = df.reset_index()\n",
    "    df.drop(df[df['ticker'].apply(lambda x: x in new_listed)].index, inplace=True)\n",
    "    df = df.set_index(['ticker', 'endDate'])\n",
    "\n",
    "    return set([(t, d) for t, d in set(df.index.to_list()) if list_date_dict[t] <= d])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0\n0\n86\n0\n0\n86\n"
     ]
    }
   ],
   "source": [
    "data_path = '../../data/financial_statements_202009/normalized_l2/'\n",
    "set_income = get_concat_df(data_path, 'income')\n",
    "set_cash_flow = get_concat_df(data_path, 'cash_flow')\n",
    "set_balance= get_concat_df(data_path, 'balance')\n",
    "\n",
    "check_dirs([data_path + 'absence/'])\n",
    "\n",
    "print(len(set_income - set_cash_flow))\n",
    "with open(data_path + 'absence/income-cash_flow.txt', 'w') as f:\n",
    "    f.write('Total: {}\\n'.format(len(set_income - set_cash_flow)))\n",
    "    f.write('-' * 50 + '\\n')\n",
    "    for i in sorted(list(set_income - set_cash_flow)):\n",
    "        f.write(str(i) + '\\n')\n",
    "print(len(set_cash_flow - set_income))\n",
    "with open(data_path + 'absence/cash_flow-income.txt', 'w') as f:\n",
    "    f.write('Total: {}\\n'.format(len(set_cash_flow - set_income)))\n",
    "    f.write('-' * 50 + '\\n')\n",
    "    for i in sorted(list(set_cash_flow - set_income)):\n",
    "        f.write(str(i) + '\\n')\n",
    "print(len(set_income - set_balance))\n",
    "with open(data_path + 'absence/income-balance.txt', 'w') as f:\n",
    "    f.write('Total: {}\\n'.format(len(set_income - set_balance)))\n",
    "    f.write('-' * 50 + '\\n')\n",
    "    for i in sorted(list(set_income - set_balance)):\n",
    "        f.write(str(i) + '\\n')\n",
    "print(len(set_balance - set_income))\n",
    "with open(data_path + 'absence/balance-income.txt', 'w') as f:\n",
    "    f.write('Total: {}\\n'.format(len(set_balance - set_income)))\n",
    "    f.write('-' * 50 + '\\n')\n",
    "    for i in sorted(list(set_balance - set_income)):\n",
    "        f.write(str(i) + '\\n')\n",
    "print(len(set_balance - set_cash_flow))\n",
    "with open(data_path + 'absence/balance-cash_flow.txt', 'w') as f:\n",
    "    f.write('Total: {}\\n'.format(len(set_balance - set_cash_flow)))\n",
    "    f.write('-' * 50 + '\\n')\n",
    "    for i in sorted(list(set_balance - set_cash_flow)):\n",
    "        f.write(str(i) + '\\n')\n",
    "print(len(set_cash_flow - set_balance))\n",
    "with open(data_path + 'absence/cash_flow-balance.txt', 'w') as f:\n",
    "    f.write('Total: {}\\n'.format(len(set_cash_flow - set_balance)))\n",
    "    f.write('-' * 50 + '\\n')\n",
    "    for i in sorted(list(set_cash_flow - set_balance)):\n",
    "        f.write(str(i) + '\\n')\n",
    "\n",
    "# data_path = '../../data/financial_statements_pit/normalized_l2/'\n",
    "# set_income = get_concat_df(data_path, 'income', '_pit')\n",
    "# set_cash_flow = get_concat_df(data_path, 'cash_flow', '_pit')\n",
    "# set_balance= get_concat_df(data_path, 'balance', '_pit')\n",
    "\n",
    "# check_dirs([data_path + 'absence/'])\n",
    "\n",
    "# print(len(set_income - set_cash_flow))\n",
    "# with open(data_path + 'absence/income-cash_flow.txt', 'w') as f:\n",
    "#     f.write('Total: {}\\n'.format(len(set_income - set_cash_flow)))\n",
    "#     f.write('-' * 50 + '\\n')\n",
    "#     for i in sorted(list(set_income - set_cash_flow)):\n",
    "#         f.write(str(i) + '\\n')\n",
    "# print(len(set_cash_flow - set_income))\n",
    "# with open(data_path + 'absence/cash_flow-income.txt', 'w') as f:\n",
    "#     f.write('Total: {}\\n'.format(len(set_cash_flow - set_income)))\n",
    "#     f.write('-' * 50 + '\\n')\n",
    "#     for i in sorted(list(set_cash_flow - set_income)):\n",
    "#         f.write(str(i) + '\\n')\n",
    "# print(len(set_income - set_balance))\n",
    "# with open(data_path + 'absence/income-balance.txt', 'w') as f:\n",
    "#     f.write('Total: {}\\n'.format(len(set_income - set_balance)))\n",
    "#     f.write('-' * 50 + '\\n')\n",
    "#     for i in sorted(list(set_income - set_balance)):\n",
    "#         f.write(str(i) + '\\n')\n",
    "# print(len(set_balance - set_income))\n",
    "# with open(data_path + 'absence/balance-income.txt', 'w') as f:\n",
    "#     f.write('Total: {}\\n'.format(len(set_balance - set_income)))\n",
    "#     f.write('-' * 50 + '\\n')\n",
    "#     for i in sorted(list(set_balance - set_income)):\n",
    "#         f.write(str(i) + '\\n')\n",
    "# print(len(set_balance - set_cash_flow))\n",
    "# with open(data_path + 'absence/balance-cash_flow.txt', 'w') as f:\n",
    "#     f.write('Total: {}\\n'.format(len(set_balance - set_cash_flow)))\n",
    "#     f.write('-' * 50 + '\\n')\n",
    "#     for i in sorted(list(set_balance - set_cash_flow)):\n",
    "#         f.write(str(i) + '\\n')\n",
    "# print(len(set_cash_flow - set_balance))\n",
    "# with open(data_path + 'absence/cash_flow-balance.txt', 'w') as f:\n",
    "#     f.write('Total: {}\\n'.format(len(set_cash_flow - set_balance)))\n",
    "#     f.write('-' * 50 + '\\n')\n",
    "#     for i in sorted(list(set_cash_flow - set_balance)):\n",
    "#         f.write(str(i) + '\\n')"
   ]
  },
  {
   "source": [
    "# Check data continuity in three statements\n",
    "***"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from os.path import join\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Check if directories exit or not\n",
    "def check_dirs(path_list):\n",
    "    for dir_path in path_list:\n",
    "        if not os.path.isdir(dir_path):\n",
    "            os.makedirs(dir_path)\n",
    "\n",
    "def check_continuity_year(df, data_path, file_name):\n",
    "\n",
    "    df['ticker'] = df['ticker'].apply(lambda x: str(x).zfill(6))\n",
    "    df = df[df['endDate'].apply(lambda x: str(x)[-6:]=='-12-31')]\n",
    "    df.drop_duplicates(subset=['ticker', 'endDate'], keep='last', inplace=True)\n",
    "    df['endDate'] = df['endDate'].apply(lambda x: int(x[:4]))\n",
    "\n",
    "    df.set_index(['endDate', 'ticker'], inplace=True)\n",
    "    df.index.names = [None, 'TICKER']\n",
    "\n",
    "    df_r = df[['reportType']]\n",
    "    df_r = df_r.unstack(1).T\n",
    "    df_r.reset_index(inplace=True)\n",
    "    df_r = df_r.drop(columns=['level_0'])\n",
    "    df_r.set_index('TICKER', inplace=True)\n",
    "\n",
    "    df_r_count_list = pd.DataFrame(columns=('TICKER', 'COUNTS', 'YEARS'))\n",
    "    df_r_not_ct_list = pd.DataFrame(columns=('TICKER', 'YEARS'))\n",
    "    last_year = int(sorted(df_r.columns)[-1])\n",
    "\n",
    "    for ticker, row in df_r.iterrows():\n",
    "        row = row.notnull()\n",
    "        counts = sum(row)\n",
    "        years_list = []\n",
    "        for k in row.keys():\n",
    "            if row[k]:\n",
    "                years_list.append(k)\n",
    "        df_r_count_list = df_r_count_list.append(\n",
    "            pd.DataFrame({'TICKER': [ticker], 'COUNTS': [counts], 'YEARS': [years_list]}), ignore_index=True)\n",
    "\n",
    "        if not years_list:\n",
    "            print('No data:', ticker)\n",
    "            df_r_not_ct_list = df_r_not_ct_list.append(\n",
    "                pd.DataFrame({'TICKER': [ticker], 'YEARS': [[]]}), ignore_index=True)\n",
    "        # elif years_list[-1] - years_list[0] != counts - 1:\n",
    "        elif last_year - years_list[0] != counts - 1:\n",
    "            df_r_not_ct_list = df_r_not_ct_list.append(\n",
    "                pd.DataFrame({'TICKER': [ticker], 'YEARS': [years_list]}), ignore_index=True)\n",
    "\n",
    "    save_path = join(data_path, 'continuity')\n",
    "    check_dirs([save_path])\n",
    "    df_r_count_list.to_excel(join(save_path, file_name + '_counts.xlsx'), index=False)\n",
    "    df_r_not_ct_list.to_excel(join(save_path, file_name + '_discontinuous.xlsx'), index=False)\n",
    "\n",
    "def check_continuity_quarter(df, data_path, file_name):\n",
    "\n",
    "    df['ticker'] = df['ticker'].apply(lambda x: str(x).zfill(6))\n",
    "    df.drop_duplicates(subset=['ticker', 'endDate'], keep='last', inplace=True)\n",
    "    df['endDate'] = df['endDate'].apply(lambda x: x[:7])\n",
    "\n",
    "    df.set_index(['endDate', 'ticker'], inplace=True)\n",
    "    df.index.names = [None, 'TICKER']\n",
    "\n",
    "    df_r = df[['reportType']]\n",
    "    df_r = df_r.unstack(1).T\n",
    "    df_r.reset_index(inplace=True)\n",
    "    df_r = df_r.drop(columns=['level_0'])\n",
    "    df_r.set_index('TICKER', inplace=True)\n",
    "\n",
    "    df_r_count_list = pd.DataFrame(columns=('TICKER', 'COUNTS', 'QUARTERS'))\n",
    "    df_r_not_ct_list = pd.DataFrame(columns=('TICKER', 'QUARTERS'))\n",
    "    quarters_all = sorted(df_r.columns)\n",
    "    last_quarter = quarters_all[-1]\n",
    "\n",
    "    for ticker, row in df_r.iterrows():\n",
    "        row = row.notnull()\n",
    "        counts = sum(row)\n",
    "        quarters_list = []\n",
    "        for k in row.keys():\n",
    "            if row[k]:\n",
    "                quarters_list.append(k)\n",
    "        df_r_count_list = df_r_count_list.append(\n",
    "            pd.DataFrame({'TICKER': [ticker], 'COUNTS': [counts], 'QUARTERS': [quarters_list]}), ignore_index=True)\n",
    "\n",
    "        if not quarters_list:\n",
    "            print('No data:', ticker)\n",
    "            df_r_not_ct_list = df_r_not_ct_list.append(\n",
    "                pd.DataFrame({'TICKER': [ticker], 'QUARTERS': [[]]}), ignore_index=True)\n",
    "        # elif quarters_list[-1] - quarters_list[0] != counts - 1:\n",
    "        elif len(quarters_all) - quarters_all.index(quarters_list[0]) != counts:\n",
    "            df_r_not_ct_list = df_r_not_ct_list.append(\n",
    "                pd.DataFrame({'TICKER': [ticker], 'QUARTERS': [quarters_list]}), ignore_index=True)\n",
    "\n",
    "    save_path = join(data_path, 'continuity')\n",
    "    check_dirs([save_path])\n",
    "    df_r_count_list.to_excel(join(save_path, file_name + '_counts.xlsx'), index=False)\n",
    "    df_r_not_ct_list.to_excel(join(save_path, file_name + '_discontinuous.xlsx'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = '20210417'\n",
    "df_info = pd.read_csv('../../data/stocks_information/stocks_info_{}.csv'.format(date), dtype={'ticker': str})\n",
    "\n",
    "new_listed = set(df_info[(df_info['listStatusCD'] == 'UN') | (df_info['listStatusCD'] == 'O')]['ticker'])\n",
    "new_listed = [i for i in new_listed if i[0] not in ['A', '9', '2']]\n",
    "\n",
    "list_date_dict = {}\n",
    "for _, row in df_info.iterrows():\n",
    "    list_date_dict[row['ticker']] = row['listDate']\n",
    "\n",
    "def get_concat_df(data_path, name, post_fix=''):\n",
    "    df_1 = pd.read_csv(join(join(data_path, 'csv'), name + '_bank' + post_fix + '.csv'))[['ticker', 'endDate', 'reportType']]\n",
    "    df_2 = pd.read_csv(join(join(data_path, 'csv'), name + '_insurance' + post_fix + '.csv'))[['ticker', 'endDate', 'reportType']]\n",
    "    df_3 = pd.read_csv(join(join(data_path, 'csv'), name + '_security' + post_fix + '.csv'))[['ticker', 'endDate', 'reportType']]\n",
    "    df_4 = pd.read_csv(join(join(data_path, 'csv'), name + '_industry' + post_fix + '.csv'))[['ticker', 'endDate', 'reportType']]\n",
    "    df = pd.concat([df_1, df_2, df_3, df_4])\n",
    "\n",
    "    df['ticker'] = df['ticker'].apply(lambda x: str(x).zfill(6))\n",
    "\n",
    "    df = df.reset_index()\n",
    "    drop_list = []\n",
    "    for i, row in df.iterrows():\n",
    "        if list_date_dict[row['ticker']] is not np.nan:\n",
    "            if list_date_dict[row['ticker']] > row['endDate']:\n",
    "                drop_list.append(i)\n",
    "\n",
    "    df.drop(drop_list, axis=0, inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "ipykernel_launcher:20: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\nipykernel_launcher:21: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "data_path = '../../data/financial_statements_202009/normalized_l2/'\n",
    "df_income = get_concat_df(data_path, 'income')\n",
    "df_cash_flow = get_concat_df(data_path, 'cash_flow')\n",
    "df_balance= get_concat_df(data_path, 'balance')\n",
    "\n",
    "check_continuity_year(df_income, data_path, 'income_year')\n",
    "check_continuity_year(df_cash_flow, data_path, 'cash_flow_year')\n",
    "check_continuity_year(df_balance, data_path, 'balance_year')\n",
    "\n",
    "check_continuity_quarter(df_income, data_path, 'income_quarter')\n",
    "check_continuity_quarter(df_cash_flow, data_path, 'cash_flow_quarter')\n",
    "check_continuity_quarter(df_balance, data_path, 'balance_quarter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "ipykernel_launcher:20: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\nipykernel_launcher:21: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "data_path = '../../data/financial_statements_202009/normalized_l2/'\n",
    "df_income = get_concat_df(data_path, 'income')\n",
    "df_cash_flow = get_concat_df(data_path, 'cash_flow')\n",
    "df_balance= get_concat_df(data_path, 'balance')\n",
    "df_all = pd.concat([df_income, df_cash_flow, df_balance])\n",
    "check_continuity_year(df_all, data_path, 'all_year')\n",
    "check_continuity_quarter(df_all, data_path, 'all_quarter')"
   ]
  },
  {
   "source": [
    "# Check duplicates\n",
    "***"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from os.path import join\n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Check if directories exit or not\n",
    "def check_dirs(path_list):\n",
    "    for dir_path in path_list:\n",
    "        if not os.path.isdir(dir_path):\n",
    "            os.makedirs(dir_path)\n",
    "\n",
    "def get_concat_df(data_path, name):\n",
    "    def _load_csv(category):\n",
    "        df = pd.read_csv(join(data_path, name + '_' + category + '.csv'), dtype={'ticker': str})\n",
    "        df['from'] = [category] * df.shape[0]\n",
    "        return df\n",
    "    df_1 = _load_csv('bank')\n",
    "    df_2 = _load_csv('insurance')\n",
    "    df_3 = _load_csv('security')\n",
    "    df_4 = _load_csv('industry')\n",
    "    df = pd.concat([df_1, df_2, df_3, df_4])\n",
    "    df = df.reset_index()\n",
    "    df.drop(columns=['index'], inplace=True)\n",
    "    return df\n",
    "\n",
    "def check_duplicates(data_path, statement_name):\n",
    "    df = get_concat_df(join(data_path, 'csv'), statement_name)\n",
    "\n",
    "    selected = df.groupby(['ticker', 'endDate']).apply(lambda x: tuple(x.index))\n",
    "\n",
    "    idx_dup = []\n",
    "    for i in selected.values:\n",
    "        if len(i) > 1:\n",
    "            idx_dup.extend(i)\n",
    "    df_dup = df.loc[idx_dup]\n",
    "\n",
    "    col_name = df_dup.columns.tolist()\n",
    "    list1 = ['ticker', 'secShortName', 'from', 'endDate', 'endDateRep', 'actPubtime',  'reportType'] \n",
    "    df_dup = df_dup.reindex(columns=list1 + [i for i in col_name if i not in list1])\n",
    "\n",
    "    df_dup = df_dup.sort_values(\n",
    "        by=['ticker', 'endDate', 'from', 'endDateRep', 'actPubtime'], \n",
    "        ascending=[True, True, True, True, True]\n",
    "    )\n",
    "\n",
    "    save_path = join(data_path, 'duplicates')\n",
    "    check_dirs([save_path])\n",
    "    df_dup.to_excel(join(save_path, statement_name) + '.xlsx', index=False)\n",
    "    print(statement_name + ':', df_dup.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "income: (72, 92)\n",
      "balance: (20, 177)\n",
      "cash_flow: (122, 89)\n"
     ]
    }
   ],
   "source": [
    "data_path = '../../data/financial_statements_202009/normalized_l2'\n",
    "check_duplicates(data_path, 'income')\n",
    "check_duplicates(data_path, 'balance')\n",
    "check_duplicates(data_path, 'cash_flow')"
   ]
  },
  {
   "source": [
    "# Get all stocks list    "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from os.path import join\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_concat_df(data_path, name):\n",
    "    def _load_csv(category):\n",
    "        df = pd.read_csv(join(data_path, name + '_' + category + '.csv'), dtype={'ticker': str})\n",
    "        return df\n",
    "    df_1 = _load_csv('bank')[['ticker', 'secShortName']]\n",
    "    df_2 = _load_csv('insurance')[['ticker', 'secShortName']]\n",
    "    df_3 = _load_csv('security')[['ticker', 'secShortName']]\n",
    "    df_4 = _load_csv('industry')[['ticker', 'secShortName']]\n",
    "    df = pd.concat([df_1, df_2, df_3, df_4])\n",
    "    return df"
   ]
  },
  {
   "source": [
    "### Companies in statements"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../../data/financial_statements/normalized_l2'\n",
    "df_income = get_concat_df(join(data_path, 'csv'), 'income')\n",
    "df_balance = get_concat_df(join(data_path, 'csv'), 'balance')\n",
    "df_cash_flow = get_concat_df(join(data_path, 'csv'), 'cash_flow')\n",
    "df = pd.concat([df_income, df_balance, df_cash_flow])\n",
    "set_s = set(df['ticker'])\n",
    "dict_s = {t: n for t, n in zip(df['ticker'], df['secShortName'])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "4307"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "len(set_s)"
   ]
  },
  {
   "source": [
    "### Listed time >= 20180101 and Unlisted"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_info = pd.read_csv('../../data/stocks_information/stocks_info_20210417.csv', dtype={'ticker': str})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_listed = set(df_info[(df_info['listDate'] >= '2018-01-01') | (df_info['listStatusCD'] == 'UN') | (df_info['listStatusCD'] == 'O')]['ticker'])"
   ]
  },
  {
   "source": [
    "### Companies in classification_fixed"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cls = pd.read_csv('../../data/classification/classification_fixed.csv', dtype={'TICKER_SYMBOL': str})\n",
    "set_c = set(df_cls['TICKER_SYMBOL'])\n",
    "dict_c = {t: n for t, n in zip(df_cls['TICKER_SYMBOL'], df_cls['STOCK_NAME'])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "3521"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "len(set_c)"
   ]
  },
  {
   "source": [
    "### Companies in cal_now_values"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../data/classification/old/cal_now_values.csv', dtype={'TICKER_SYMBOL': str})\n",
    "df['ticker'] = df['TICKER_SYMBOL'].apply(lambda x: str(x).zfill(6))\n",
    "set_o = set(df['ticker'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "3521"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "len(set_o)"
   ]
  },
  {
   "source": [
    "### Get intersection"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "('001872', '招商港口')\n('001914', '招商积余')\n('002808', '恒久科技')\n"
     ]
    }
   ],
   "source": [
    "for i in sorted([(i, dict_s[i]) for i in set_s - new_listed - set_c]):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_path + '/absence/all_tickers-classification.txt', 'w') as f:\n",
    "    f.write('Total: {}\\n'.format(len(set_s - new_listed - set_c)))\n",
    "    f.write('-' * 50 + '\\n')\n",
    "    for i in sorted([(i, dict_s[i]) for i in set_s - new_listed - set_c]):\n",
    "        f.write(str(i) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "3429"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "len(set_s - new_listed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "set_o - set_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../data/standard_dataset/sheet_2/csv/income_bank.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "income bank netExpHIncome\nincome bank quitConcernNi\nincome bank othEffectNpp\nincome bank othEffectPci\nincome bank premEarned\nincome bank grossPremWrit\nincome bank reinsIncome\nincome bank reinsur\nincome bank unePremReser\nincome bank premRefund\nincome bank compensPayout\nincome bank compensPayoutRefu\nincome bank reserInsurLiab\nincome bank insurLiabReserRefu\nincome bank policyDivPayt\nincome bank reinsurExp\nincome bank reinsCostRefund\nincome bank nSecTaIncome\nincome bank nUndwrtSecIncome\nincome bank nTrustIncome\nincome bank bizTaSurchg\nincome bank tRevenue\nincome bank tCogs\nincome bank nCompensPayout\nincome bank reserInsurContr\nincome bank sellExp\nincome bank adminExp\nincome bank rDExp\nincome bank finanExp\nincome bank intExpFinanExp\nincome bank intIncomeFinanExp\nincome bank specToc\nincome bank ATOC\nincome bank ncaDisploss\nincome bank nIncomeBma\n"
     ]
    }
   ],
   "source": [
    "type = 'income'\n",
    "cls = 'bank'\n",
    "# Drop null columns\n",
    "for col, n in df.isnull().all().iteritems():\n",
    "    if n:\n",
    "        print(type, cls, col)\n",
    "    df = df.drop(columns=[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}